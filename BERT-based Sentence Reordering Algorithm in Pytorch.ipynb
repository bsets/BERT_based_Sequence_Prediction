{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import sys\n",
    "import torch\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import transformers\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Datasets\n",
    "\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    trainset = pickle.load(f)\n",
    "\n",
    "with open('test.pkl', 'rb') as f:\n",
    "    testset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list 'indexes' and store the order of sentences of each example in the trainset in this list.\n",
    "# So this is a list of lists\n",
    "\n",
    "indexes = []\n",
    "for instance in trainset:\n",
    "  indexes.append(instance['indexes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT Tokenizer (I have used the bert-base-uncased as bert-large was not loading on my machine)\n",
    "\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenization for the training set sentences that can be then fed as input to BERT\n",
    "\n",
    "def bert_tokenization(dataset):\n",
    "\n",
    "  input_ids = []\n",
    "  labels = []\n",
    "  attention_masks = []\n",
    "  token_type_ids = []\n",
    "\n",
    "  counter = 0\n",
    "  for instance in dataset:\n",
    "    if counter % 1000 == 0: print('current step is =: ', counter)\n",
    "    counter = counter + 1 \n",
    "\n",
    "    indexes = instance['indexes']\n",
    "    inputs_set = []\n",
    "    labels_set = []\n",
    "    attention_mask_set = []\n",
    "    token_type_id_set = []\n",
    "    for i in range(0,6):\n",
    "      for j in range(i+1,6):\n",
    "        s1 = instance['sentences'][i]\n",
    "        s2 = instance['sentences'][j]\n",
    "        encoded_sentences = tokenizer.encode_plus(s1, s2, \n",
    "                          max_length = 100, \n",
    "                          truncation= True,\n",
    "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                          return_attention_mask = True,\n",
    "                          return_token_type_ids = True,\n",
    "                          pad_to_max_length = True,\n",
    "                          return_tensors = 'pt')\n",
    "        input = encoded_sentences['input_ids']\n",
    "        attention_mask = encoded_sentences['attention_mask']\n",
    "        token_type_id = encoded_sentences['token_type_ids']\n",
    "\n",
    "        inputs_set.append(input)\n",
    "        attention_mask_set.append(attention_mask)\n",
    "        token_type_id_set.append(token_type_id)\n",
    "        index1 = indexes[i]\n",
    "        index2 = indexes[j]\n",
    "        if (index1 < index2):\n",
    "          label = 1\n",
    "        else: \n",
    "          label = 0\n",
    "        labels_set.append(label)\n",
    "    input_ids.append(inputs_set)\n",
    "    labels.append(labels_set)\n",
    "    attention_masks.append(attention_mask_set)\n",
    "    token_type_ids.append(token_type_id_set)\n",
    "  return input_ids, attention_masks, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, I have selected 60000 sentence samples from the train.pkl dataset to train the model \n",
    "\n",
    "n_size = 60000\n",
    "\n",
    "# The tokens are passed to the variables input_ids, attention_masks, token_type_ids, labels\n",
    "\n",
    "input_ids, attention_masks, token_type_ids, labels = bert_tokenization(trainset[0:n_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, the training sentences and labels are converted to Pytorch format \n",
    "\n",
    "# convert input_ids \n",
    "for each in input_ids:\n",
    "  each = torch.cat(each, dim=0)\n",
    "b = torch.zeros([len(input_ids), 15, 100], dtype = torch.int32)\n",
    "for i in range(0,len(input_ids)):\n",
    "  for j in range(0,15):\n",
    "    b[i][j] = input_ids[i][j]\n",
    "input_ids = b\n",
    "#print(input_ids.shape) # torch.Size([1000, 15, 100])\n",
    "\n",
    "# convert labels\n",
    "c = torch.zeros([len(labels), 15], dtype = torch.int64)\n",
    "for i in range(0,len(labels)):\n",
    "  c[i] = torch.Tensor(labels[i])\n",
    "labels = c\n",
    "#print(labels.shape) # torch.Size([1000, 15, 100])\n",
    "\n",
    "# convert attention_masks\n",
    "c = torch.zeros([len(attention_masks), 15, 100], dtype = torch.int32)\n",
    "for i in range(0,len(attention_masks)):\n",
    "  for j in range(0,15):\n",
    "    c[i][j] = attention_masks[i][j]\n",
    "attention_masks = c\n",
    "#print(attention_masks.shape) # torch.Size([1000, 15, 100])\n",
    "\n",
    "# convert token type ids\n",
    "c = torch.zeros([len(token_type_ids), 15, 100], dtype = torch.int32)\n",
    "for i in range(0,len(token_type_ids)):\n",
    "  for j in range(0,15):\n",
    "    c[i][j] = token_type_ids[i][j]\n",
    "token_type_ids = c\n",
    "#print(token_type_ids.shape) # torch.Size([1000, 15, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokens are reshaped to 60000*15 times 100.\n",
    "\n",
    "input_ids = input_ids.reshape(n_size*15,100)\n",
    "attention_masks = attention_masks.reshape(n_size*15,100)\n",
    "token_type_ids = token_type_ids.reshape(n_size*15,100)\n",
    "labels = labels.reshape(n_size*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get 900000 sentence pairs of token length 100 each\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_masks.shape)\n",
    "print(token_type_ids.shape) \n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training dataset has been split into a training set and a validation set \n",
    "\n",
    "set_train_size = int(0.9 * n_size)\n",
    "train_size = set_train_size *15\n",
    "val_size = (n_size - set_train_size)*15\n",
    "\n",
    "print(train_size/15, val_size/15, train_size/15 + val_size/15)\n",
    "print(train_size, val_size, train_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training and the validation sets have been converted into PyTorch format\n",
    "\n",
    "train_dataset = TensorDataset(input_ids[:train_size], \n",
    "                              attention_masks[:train_size], \n",
    "                              token_type_ids[:train_size], \n",
    "                              labels[:train_size])\n",
    "val_dataset = TensorDataset(input_ids[train_size:], \n",
    "                            attention_masks[train_size:], \n",
    "                            token_type_ids[train_size:], \n",
    "                            labels[train_size:])\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step performs BERT tokenization for the test dataset \n",
    "\n",
    "def bert_tokenization2(dataset):\n",
    "  \n",
    "  input_ids = []\n",
    "  \n",
    "  attention_masks = []\n",
    "  token_type_ids = []\n",
    "\n",
    "  counter = 0\n",
    "  for instance in dataset:\n",
    "    if counter % 1000 == 0: print('current step is =: ', counter)\n",
    "    counter = counter + 1 \n",
    "\n",
    "    \n",
    "    inputs_set = []\n",
    "    \n",
    "    attention_mask_set = []\n",
    "    token_type_id_set = []\n",
    "    for i in range(0,6):\n",
    "      for j in range(i+1,6):\n",
    "        s1 = instance['sentences'][i]\n",
    "        s2 = instance['sentences'][j]\n",
    "        encoded_sentences = tokenizer.encode_plus(s1, s2, \n",
    "                          max_length = 100, \n",
    "                          truncation= True,\n",
    "                          add_special_tokens = True, \n",
    "                          return_attention_mask = True,\n",
    "                          return_token_type_ids = True,\n",
    "                          pad_to_max_length = True,\n",
    "                          return_tensors = 'pt')\n",
    "        input = encoded_sentences['input_ids']\n",
    "        attention_mask = encoded_sentences['attention_mask']\n",
    "        token_type_id = encoded_sentences['token_type_ids']\n",
    "\n",
    "        inputs_set.append(input)\n",
    "        attention_mask_set.append(attention_mask)\n",
    "        token_type_id_set.append(token_type_id)\n",
    "\n",
    "    input_ids.append(inputs_set)\n",
    "    \n",
    "    attention_masks.append(attention_mask_set)\n",
    "    token_type_ids.append(token_type_id_set)\n",
    "  return input_ids, attention_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once again, the variables input_ids1, attention_masks1, token_type_ids1 store the results of the tokenization \n",
    "# step\n",
    "input_ids1, attention_masks1, token_type_ids1 = bert_tokenization2(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step performs conversion to Pytorch of the test dataset\n",
    "\n",
    "# input ids to torch format\n",
    "\n",
    "for each in input_ids1:\n",
    "  each = torch.cat(each, dim=0)\n",
    "b = torch.zeros([len(input_ids1), 15, 100], dtype = torch.int32)\n",
    "for i in range(0,len(input_ids1)):\n",
    "  for j in range(0,15):\n",
    "    b[i][j] = input_ids1[i][j]\n",
    "input_ids1 = b\n",
    "print(input_ids1.shape) \n",
    "\n",
    "\n",
    "# attention_masks to torch format\n",
    "c = torch.zeros([len(attention_masks1), 15, 100], dtype = torch.int32)\n",
    "for i in range(0,len(attention_masks1)):\n",
    "  for j in range(0,15):\n",
    "    c[i][j] = attention_masks1[i][j]\n",
    "attention_masks1 = c\n",
    "print(attention_masks1.shape) \n",
    "\n",
    "# token type ids to torch format\n",
    "\n",
    "c = torch.zeros([len(token_type_ids1), 15, 100], dtype = torch.int32)\n",
    "for i in range(0,len(token_type_ids1)):\n",
    "  for j in range(0,15):\n",
    "    c[i][j] = token_type_ids1[i][j]\n",
    "token_type_ids1 = c\n",
    "print(token_type_ids1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the algorithm is converted to 30000 by 100 format\n",
    "\n",
    "input_ids1 = input_ids1.reshape(2000*15,100)\n",
    "attention_masks1 = attention_masks1.reshape(2000*15,100)\n",
    "token_type_ids1 = token_type_ids1.reshape(2000*15,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tensor format for the test dataset is prepared by this step\n",
    "\n",
    "test_dataset = TensorDataset(input_ids1, attention_masks1, token_type_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader has been prepared for the train and validation set with a batch size of 16\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "           batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used BertforSentencePrediction \n",
    "\n",
    "#from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import BertForNextSentencePrediction, AdamW, BertConfig\n",
    "# from transformers import RobertaForSequenceClassification\n",
    "\n",
    "#model = BertForSequenceClassification.from_pretrained(\n",
    "model = BertForNextSentencePrediction.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    #\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_input_tokentypeids = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=b_input_tokentypeids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_input_tokentypeids = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=b_input_tokentypeids, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "#prediction_data = TensorDataset(input_ids1, attention_masks1)\n",
    "#prediction_sampler = SequentialSampler(prediction_data)\n",
    "#prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "    #               Test\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Validation on toy set...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "label = np.array([])\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack this training batch from our dataloader. \n",
    "    #\n",
    "    # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "    # the `to` method.\n",
    "    #\n",
    "    # `batch` contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids':      batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'token_type_ids': batch[2]\n",
    "          }\n",
    "     \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        result = model(**inputs)\n",
    "\n",
    "    # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "    # output values prior to applying an activation function like the \n",
    "    # softmax.\n",
    "    loss = result.loss\n",
    "    logits = result.logits\n",
    "        \n",
    "    # Accumulate the validation loss.\n",
    "    # total_eval_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    label = np.append(label, pred_flat)\n",
    "\n",
    "    # label_ids =inputs['labels'].to('cpu').numpy()\n",
    "    \n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences, and\n",
    "    # accumulate it over all batches.\n",
    "    # total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "# avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "# print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "# print(\"  Validation took: {:}\".format(validation_time))\n",
    "test_label = label.reshape(2000,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topSort(data):\n",
    "  orders = []\n",
    "  for instance in data:\n",
    "    g = Graph(6)\n",
    "    for j in range(0,15):\n",
    "      pred = instance[j]\n",
    "      pos_s1, pos_s2 = get_pos(j)\n",
    "\n",
    "      if pred == 1: \n",
    "        g.addEdge(pos_s1, pos_s2)\n",
    "      if pred == 0: \n",
    "        g.addEdge(pos_s2, pos_s1)\n",
    "    while g.isCyclic():\n",
    "      g.isCyclic()\n",
    "\n",
    "    sorted = g.topologicalSort()\n",
    "    arr = []\n",
    "    for i in range(0,6):\n",
    "      arr.append(sorted.index(i))\n",
    "  \n",
    "    orders.append(arr)\n",
    "  return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_order = topSort(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pdct to submit.\n",
    "k = {'ID': range(1,2001), \n",
    "     'index1': range(2000),\n",
    "     'index2': range(2000),\n",
    "     'index3': range(2000),\n",
    "     'index4': range(2000),\n",
    "     'index5': range(2000),\n",
    "     'index6': range(2000)}\n",
    "\n",
    "df = pd.DataFrame(data=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2000):\n",
    "  instance = sentence_order[i]\n",
    "  for j in range(1,7):\n",
    "    df.iloc[i,j]=instance[j-1]\n",
    "df.to_csv(\"/home/bharat/Desktop/......./sentence_order\",index=False) # Directory to store the sentence_order file  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
